{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Weather DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate station data by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "cities = [\"ATL_stations\",\"CH_stations\",\"LA_stations\",\"NYC_stations\",\"SD_stations\",\"SF_stations\"]\n",
    "#Aggregate Station Location tuples \n",
    "stations_data = pd.DataFrame()\n",
    "for city in cities:\n",
    "    path = 'station_locations/%s.txt' % city\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True,header=None,error_bad_lines=False)\n",
    "        frame['city'] = city\n",
    "        stations_data = stations_data.append(frame,ignore_index=True)\n",
    "stations_data = stations_data.rename(columns={0:\"distance\",1:\"station_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ATLshortlist = stations_data[(stations_data.city == \"ATL_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "CHshortlist = stations_data[(stations_data.city == \"CH_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "LAshortlist = stations_data[(stations_data.city == \"LA_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "NYCshortlist = stations_data[(stations_data.city == \"NYC_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "SDshortlist = stations_data[(stations_data.city == \"SD_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "SFshortlist = stations_data[(stations_data.city == \"SF_stations\") & (stations_data.distance <= 100)][['station_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Load Data \n",
    "###### Merge datasets from each station with PARAM_STATION-NAME as default column header\n",
    "##### Process results in 10 stations per city\n",
    "##### UPDATE: Frame.query removes scrappy data (missing data will still exist for some!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######ATL Weather######\n",
    "ATL_stations = [];\n",
    "ATL_weather = pd.DataFrame()\n",
    "for station in ATLshortlist['station_name']:\n",
    "    path = 'ATL/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12650:\n",
    "            ATL_stations.append(station);\n",
    "            if ATL_weather.empty:\n",
    "                ATL_weather = ATL_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                ATL_weather = ATL_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "ATL_weather = ATL_weather.groupby('DATE').mean()\n",
    "ATL_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######CH Weather######\n",
    "CH_stations = [];\n",
    "CH_weather = pd.DataFrame()\n",
    "for station in CHshortlist['station_name']:\n",
    "    path = 'CH/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12764:\n",
    "            CH_stations.append(station)\n",
    "            if CH_weather.empty:\n",
    "                CH_weather = CH_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                CH_weather = CH_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "CH_weather = CH_weather.groupby('DATE').mean()\n",
    "CH_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######NYC Weather######\n",
    "NYC_stations = [];\n",
    "NYC_weather = pd.DataFrame()\n",
    "for station in NYCshortlist['station_name']:\n",
    "    path = 'NYC/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12700:\n",
    "            NYC_stations.append(station)\n",
    "            if NYC_weather.empty:\n",
    "                NYC_weather = NYC_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                NYC_weather = NYC_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "NYC_weather = NYC_weather.groupby('DATE').mean()\n",
    "NYC_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######LA Weather######\n",
    "LA_stations = [];\n",
    "LA_weather = pd.DataFrame()\n",
    "for station in LAshortlist['station_name']:\n",
    "    path = 'LA/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12600: \n",
    "            LA_stations.append(station)\n",
    "            if LA_weather.empty:\n",
    "                LA_weather = LA_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                LA_weather = LA_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "LA_weather = LA_weather.groupby('DATE').mean()\n",
    "LA_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######SF Weather######\n",
    "SF_stations = [];\n",
    "SF_weather = pd.DataFrame()\n",
    "for station in SFshortlist['station_name']:\n",
    "    path = 'SF/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12784: \n",
    "            SF_stations.append(station)\n",
    "            if SF_weather.empty:\n",
    "                SF_weather = SF_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                SF_weather = SF_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "SF_weather = SF_weather.groupby('DATE').mean()\n",
    "SF_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######SD Weather######\n",
    "SD_stations = [];\n",
    "SD_weather = pd.DataFrame()\n",
    "for station in SDshortlist['station_name']:\n",
    "    path = 'SD/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12600: \n",
    "            SD_stations.append(station)\n",
    "            if SD_weather.empty:\n",
    "                SD_weather = SD_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                SD_weather = SD_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "SD_weather = SD_weather.groupby('DATE').mean()\n",
    "SD_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Logger\n",
    "##### True indicates a missing data row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"New York City stations missing data\")\n",
    "for station in NYC_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(NYC_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Atlanta stations missing data\")\n",
    "for station in ATL_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(ATL_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "    \n",
    "print(\"San Francisco stations missing data\")\n",
    "for station in SF_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(SF_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"San Diego stations missing data\")\n",
    "for station in SD_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(SD_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Los Angeles stations missing data\")\n",
    "for station in LA_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(LA_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Chicago stations missing data\")\n",
    "for station in CH_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(CH_weather)['TMIN_'+station].value_counts())\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge Datasets from each stock with PARAM_STOCK-NAME as default columns           \n",
    "######Stock Data######\n",
    "stocks = ['Apple, Inc Stock','International Business Machines Stock','Wal-Mart Stores, Inc Common St Stock','FedEx Corporation','The Boeing Company']\n",
    "stock_data = pd.DataFrame()\n",
    "for stock in stocks:\n",
    "    path = 'Stock Data/%s.csv' % stock\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path)\n",
    "        frame.columns = ['DATE','OPEN_'+stock,'HIGH_'+stock,'LOW_'+stock,'CLOSE_'+stock,'VOLUME_'+stock,'ADJ CLOSE_'+stock]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        for column in frame.columns:\n",
    "            if column != 'DATE':\n",
    "                frame['PRE'+column] = frame[column].shift(-1)\n",
    "        if stock_data.empty:\n",
    "            stock_data = stock_data.append(frame,ignore_index=True)\n",
    "        else:\n",
    "            stock_data = stock_data.merge(frame, on='DATE', how='inner')\n",
    "stock_data = stock_data.groupby('DATE').mean()\n",
    "stock_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge Datasets from each stock with CITY-NAME_sunlight as default columns\n",
    "#####Sunlight Data######\n",
    "cities = ['ATL_sunlight','NYC_sunlight','LA_sunlight','SF_sunlight','SD_sunlight','CH_sunlight']\n",
    "sunlight_data = pd.DataFrame()\n",
    "for city in cities:\n",
    "    path = 'Sunlight_data/%s.csv' % city\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True,keep_date_col=True, header=None)\n",
    "        frame = frame.iloc[:,0:2]\n",
    "        frame.columns = ['DATE',city]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if sunlight_data.empty:\n",
    "            sunlight_data = sunlight_data.append(frame,ignore_index=True)\n",
    "        else:\n",
    "            sunlight_data = sunlight_data.merge(frame, on='DATE', how='inner')\n",
    "sunlight_data = sunlight_data.groupby('DATE').mean()\n",
    "sunlight_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "data_frames = [ATL_weather,CH_weather,NYC_weather,LA_weather,SD_weather,SF_weather,sunlight_data,stock_data]\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='inner',suffixes=('','_y'), on='DATE'), data_frames)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TMAX_AVG = [0 for x in range(len(df_final.index))]\n",
    "zipped = zip(list(df_final.columns.values),TMAX_AVG)\n",
    "count=0\n",
    "for i in list(df_final.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_AVG=TMAX_AVG+df_final.ix[:,i]\n",
    "\n",
    "\n",
    "################################################333\n",
    "TMAX_ATL = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_ATL=TMAX_ATL+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_ATL= TMAX_ATL/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_CH = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_CH=TMAX_CH+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_CH= TMAX_CH/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMAX_LA = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_LA=TMAX_LA+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_LA= TMAX_LA/count\n",
    "\n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_NYC = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_NYC=TMAX_NYC+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_NYC= TMAX_NYC/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMAX_SD = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_SD=TMAX_SD+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_SD= TMAX_SD/count\n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_SF = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_SF=TMAX_SF+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_SF= TMAX_SF/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_ATL = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_ATL=TMIN_ATL+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_ATL= TMIN_ATL/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_CH = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_CH=TMIN_CH+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_CH= TMIN_CH/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_LA = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_LA=TMIN_LA+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_LA= TMIN_LA/count\n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_NYC = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_NYC=TMIN_NYC+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_NYC= TMIN_NYC/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMIN_SD = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_SD=TMIN_SD+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_SD= TMIN_SD/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_SF = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_SF=TMIN_SF+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_SF= TMIN_SF/count \n",
    "###################################################3\n",
    "##################################################\n",
    "\n",
    "SNOW_ATL = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_ATL=SNOW_ATL+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_ATL= SNOW_ATL/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNOW_NYC = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_NYC=SNOW_NYC+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_NYC= SNOW_NYC/count \n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_CH = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_CH=SNOW_CH+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_CH= SNOW_CH/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_LA = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_LA=SNOW_LA+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_LA= SNOW_LA/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_SD = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_SD=SNOW_SD+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_SD= SNOW_SD/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_SF = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_SF=SNOW_SF+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_SF= SNOW_SF/count \n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNWD_ATL = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_ATL=SNWD_ATL+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_ATL= SNWD_ATL/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_NYC = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_NYC=SNWD_NYC+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_NYC= SNWD_NYC/count\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_CH = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_CH=SNWD_CH+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_CH= SNWD_CH/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_LA = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_LA=SNWD_LA+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_LA= SNWD_LA/count \n",
    "\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_SD = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_SD=SNWD_SD+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_SD= SNWD_SD/count \n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "SNWD_SF = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_SF=SNWD_SF+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_SF= SNWD_SF/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "PRCP_NYC = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_NYC=PRCP_NYC+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_NYC= PRCP_NYC/count\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_ATL = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_ATL=PRCP_ATL+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_ATL= PRCP_ATL/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_CH = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_CH=PRCP_CH+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_CH= PRCP_CH/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_LA = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_LA=PRCP_LA+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_LA= PRCP_LA/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_SD = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_SD=PRCP_SD+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_SD= PRCP_SD/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_SF = [0 for x in range(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_SF=PRCP_SF+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_SF= PRCP_SF/count \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"TMAX_ATL\"]=TMAX_AVG_ATL\n",
    "df[\"TMAX_NYC\"]=TMAX_AVG_NYC\n",
    "df[\"TMAX_CH\"]=TMAX_AVG_CH\n",
    "df[\"TMAX_LA\"]=TMAX_AVG_LA\n",
    "df[\"TMAX_SD\"]=TMAX_AVG_SD\n",
    "df[\"TMAX_SF\"]=TMAX_AVG_SF\n",
    "\n",
    "\n",
    "df[\"TMIN_ATL\"]=TMIN_AVG_ATL\n",
    "df[\"TMIN_NYC\"]=TMIN_AVG_NYC\n",
    "df[\"TMIN_CH\"]=TMIN_AVG_CH\n",
    "df[\"TMIN_LA\"]=TMIN_AVG_LA\n",
    "df[\"TMIN_SD\"]=TMIN_AVG_SD\n",
    "df[\"TMIN_SF\"]=TMIN_AVG_SF \n",
    "\n",
    "df[\"SNOW_ATL\"]=SNOW_AVG_ATL\n",
    "df[\"SNOW_NYC\"]=SNOW_AVG_NYC\n",
    "df[\"SNOW_CH\"]=SNOW_AVG_CH\n",
    "df[\"SNOW_LA\"]=SNOW_AVG_LA\n",
    "df[\"SNOW_SD\"]=SNOW_AVG_SD\n",
    "df[\"SNOW_SF\"]=SNOW_AVG_SF \n",
    "\n",
    "df[\"SNWD_ATL\"]=SNWD_AVG_ATL\n",
    "df[\"SNWD_NYC\"]=SNWD_AVG_NYC\n",
    "df[\"SNWD_CH\"]=SNWD_AVG_CH\n",
    "df[\"SNWD_LA\"]=SNWD_AVG_LA\n",
    "df[\"SNWD_SD\"]=SNWD_AVG_SD\n",
    "df[\"SNWD_SF\"]=SNWD_AVG_SF \n",
    "\n",
    "df[\"PRCP_ATL\"]=PRCP_AVG_ATL\n",
    "df[\"PRCP_NYC\"]=PRCP_AVG_NYC\n",
    "df[\"PRCP_CH\"]=PRCP_AVG_CH\n",
    "df[\"PRCP_LA\"]=PRCP_AVG_LA\n",
    "df[\"PRCP_SD\"]=PRCP_AVG_SD\n",
    "df[\"PRCP_SF\"]=PRCP_AVG_SF \n",
    "\n",
    "df[\"SUN_ATL\"]=df_final[\"ATL_sunlight\"]\n",
    "df[\"SUN_NYC\"]=df_final[\"NYC_sunlight\"]\n",
    "df[\"SUN_LA\"]=df_final[\"LA_sunlight\"]\n",
    "df[\"SUN_CH\"]=df_final[\"CH_sunlight\"]\n",
    "df[\"SUN_SD\"]=df_final[\"SD_sunlight\"]\n",
    "df[\"SUN_SF\"]=df_final[\"SF_sunlight\"]\n",
    "\n",
    "##Adding day column##\n",
    "from datetime import date, datetime\n",
    "import calendar\n",
    "date.today().strftime(\"%A\")\n",
    "dx = {}\n",
    "for date in df_final[\"DATE\"]:\n",
    "        dx[date] = date.strftime(\"%A\")\n",
    "import collections\n",
    "dx = collections.OrderedDict(sorted(dx.items()))\n",
    "df_final[\"DAY\"] = dx.values()\n",
    "df_final[\"DAY\"] = df_final[\"DAY\"].astype('category')\n",
    "df = pd.concat([df, pd.get_dummies(df_final[\"DAY\"])],axis =1) \n",
    "####\n",
    "\n",
    "#df = pd.concat([df, df_final[\"DAY\"]], axis=1)\n",
    "\n",
    "df[\"PreWalMart\"]= (-np.log(df_final[\"PREOPEN_Wal-Mart Stores, Inc Common St Stock\"])+ np.log(df_final[\"PRECLOSE_Wal-Mart Stores, Inc Common St Stock\"]))*100*df_final[\"PREVOLUME_Wal-Mart Stores, Inc Common St Stock\"]  \n",
    "\n",
    "df[\"PreApple\"]= (-np.log(df_final[\"PREOPEN_Apple, Inc Stock\"])+ np.log(df_final[\"PRECLOSE_Apple, Inc Stock\"]))*100*df_final[\"PREVOLUME_Apple, Inc Stock\"]\n",
    "                                                               \n",
    "df[\"PreBoeing\"]= (-np.log(df_final[\"PREOPEN_The Boeing Company\"])+ np.log(df_final[\"PRECLOSE_The Boeing Company\"]))*100*df_final[\"PREVOLUME_The Boeing Company\"]\n",
    "\n",
    "df[\"PreFedEx\"]= (-np.log(df_final[\"PREOPEN_FedEx Corporation\"])+ np.log(df_final[\"PRECLOSE_FedEx Corporation\"]))*100*df_final[\"PREVOLUME_FedEx Corporation\"]\n",
    "\n",
    "df[\"PreIBM\"]= (-np.log(df_final[\"PREOPEN_International Business Machines Stock\"])+ np.log(df_final[\"PRECLOSE_International Business Machines Stock\"]))*100*df_final[\"PREVOLUME_International Business Machines Stock\"]\n",
    "\n",
    "df[\"WalMart\"]= (-np.log(df_final[\"OPEN_Wal-Mart Stores, Inc Common St Stock\"])+ np.log(df_final[\"CLOSE_Wal-Mart Stores, Inc Common St Stock\"]))*100*df_final[\"VOLUME_Wal-Mart Stores, Inc Common St Stock\"]  \n",
    "\n",
    "df[\"Apple\"]= (-np.log(df_final[\"OPEN_Apple, Inc Stock\"])+ np.log(df_final[\"CLOSE_Apple, Inc Stock\"]))*100*df_final[\"VOLUME_Apple, Inc Stock\"]\n",
    "                                                               \n",
    "df[\"Boeing\"]= (-np.log(df_final[\"OPEN_The Boeing Company\"])+ np.log(df_final[\"CLOSE_The Boeing Company\"]))*100*df_final[\"VOLUME_The Boeing Company\"]\n",
    "\n",
    "df[\"FedEx\"]= (-np.log(df_final[\"OPEN_FedEx Corporation\"])+ np.log(df_final[\"CLOSE_FedEx Corporation\"]))*100*df_final[\"VOLUME_FedEx Corporation\"]\n",
    "\n",
    "df[\"IBM\"]= (-np.log(df_final[\"OPEN_International Business Machines Stock\"])+ np.log(df_final[\"CLOSE_International Business Machines Stock\"]))*100*df_final[\"VOLUME_International Business Machines Stock\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.describe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "import time\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "X = pd.DataFrame(df.ix[:,0:42]) #weather parameters with Pre-Walmart \n",
    "Y = pd.DataFrame(df.ix[:,46:47]) #stock data for Walmart \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y.ix[:,0], test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###UPDATE: Need to get Gridsearch to stop timing out\n",
    "parameters = {'kernel':['poly', 'rbf'], 'C':[.01,10,1000],'gamma':[.0001,.001,.01], 'degree':[3,4,5]}\n",
    "regressor = SVR()\n",
    "clf = GridSearchCV(regressor,parameters,scoring='mean_squared_error',n_jobs=4).fit(X_train,y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regressor.set_params(**clf.best_params_)\n",
    "regressor.fit(X_train, y_train)\n",
    "pred = regressor.predict(X_test)\n",
    "acc = regressor.score(X_test, y_test)\n",
    "print('ACC: %.4f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
