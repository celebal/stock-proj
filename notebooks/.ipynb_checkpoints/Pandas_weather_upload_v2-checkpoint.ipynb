{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Weather DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate station data by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 10\n",
    "cities = [\"ATL_stations\",\"CH_stations\",\"LA_stations\",\"NYC_stations\",\"SD_stations\",\"SF_stations\"]\n",
    "#Aggregate Station Location tuples \n",
    "stations_data = pd.DataFrame()\n",
    "for city in cities:\n",
    "    path = 'station_locations/%s.txt' % city\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True,header=None,error_bad_lines=False)\n",
    "        frame['city'] = city\n",
    "        stations_data = stations_data.append(frame,ignore_index=True)\n",
    "stations_data = stations_data.rename(columns={0:\"distance\",1:\"station_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ATLshortlist = stations_data[(stations_data.city == \"ATL_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "CHshortlist = stations_data[(stations_data.city == \"CH_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "LAshortlist = stations_data[(stations_data.city == \"LA_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "NYCshortlist = stations_data[(stations_data.city == \"NYC_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "SDshortlist = stations_data[(stations_data.city == \"SD_stations\") & (stations_data.distance <= 100)][['station_name']]\n",
    "SFshortlist = stations_data[(stations_data.city == \"SF_stations\") & (stations_data.distance <= 100)][['station_name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Load Data \n",
    "###### Merge datasets from each station with PARAM_STATION-NAME as default column header\n",
    "##### Process results in 10 stations per city\n",
    "##### UPDATE: Frame.query removes scrappy data (missing data will still exist for some!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######ATL Weather######\n",
    "ATL_stations = [];\n",
    "ATL_weather = pd.DataFrame()\n",
    "for station in ATLshortlist['station_name']:\n",
    "    path = 'ATL/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12650:\n",
    "            ATL_stations.append(station);\n",
    "            if ATL_weather.empty:\n",
    "                ATL_weather = ATL_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                ATL_weather = ATL_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "ATL_weather = ATL_weather.groupby('DATE').mean()\n",
    "ATL_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######CH Weather######\n",
    "CH_stations = [];\n",
    "CH_weather = pd.DataFrame()\n",
    "for station in CHshortlist['station_name']:\n",
    "    path = 'CH/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12764:\n",
    "            CH_stations.append(station)\n",
    "            if CH_weather.empty:\n",
    "                CH_weather = CH_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                CH_weather = CH_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "CH_weather = CH_weather.groupby('DATE').mean()\n",
    "CH_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######NYC Weather######\n",
    "NYC_stations = [];\n",
    "NYC_weather = pd.DataFrame()\n",
    "for station in NYCshortlist['station_name']:\n",
    "    path = 'NYC/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12700:\n",
    "            NYC_stations.append(station)\n",
    "            if NYC_weather.empty:\n",
    "                NYC_weather = NYC_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                NYC_weather = NYC_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "NYC_weather = NYC_weather.groupby('DATE').mean()\n",
    "NYC_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######LA Weather######\n",
    "LA_stations = [];\n",
    "LA_weather = pd.DataFrame()\n",
    "for station in LAshortlist['station_name']:\n",
    "    path = 'LA/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12600: \n",
    "            LA_stations.append(station)\n",
    "            if LA_weather.empty:\n",
    "                LA_weather = LA_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                LA_weather = LA_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "LA_weather = LA_weather.groupby('DATE').mean()\n",
    "LA_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######SF Weather######\n",
    "SF_stations = [];\n",
    "SF_weather = pd.DataFrame()\n",
    "for station in SFshortlist['station_name']:\n",
    "    path = 'SF/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12784: \n",
    "            SF_stations.append(station)\n",
    "            if SF_weather.empty:\n",
    "                SF_weather = SF_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                SF_weather = SF_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "SF_weather = SF_weather.groupby('DATE').mean()\n",
    "SF_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######SD Weather######\n",
    "SD_stations = [];\n",
    "SD_weather = pd.DataFrame()\n",
    "for station in SDshortlist['station_name']:\n",
    "    path = 'SD/%s.csv' % station\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True)\n",
    "        frame = frame.query('TMIN != TMAX')\n",
    "        frame.columns = ['DATE', 'TMAX_'+station,'TMIN_'+station,'SNOW_'+station,'SNWD_'+station,'PRCP_'+station]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if frame.shape[0] >= 12600: \n",
    "            SD_stations.append(station)\n",
    "            if SD_weather.empty:\n",
    "                SD_weather = SD_weather.append(frame,ignore_index=True)\n",
    "            else:\n",
    "                SD_weather = SD_weather.merge(frame, on='DATE', how='inner', suffixes=('',''))\n",
    "SD_weather = SD_weather.groupby('DATE').mean()\n",
    "SD_weather.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Logger\n",
    "##### True indicates a missing data row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York City stations missing data\n",
      "Station USC00300889\n",
      "False    12631\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00305426\n",
      "False    12631\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00094745\n",
      "False    12631\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00283951\n",
      "False    12631\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00014734\n",
      "False    12631\n",
      "dtype: int64\n",
      "()\n",
      "Atlanta stations missing data\n",
      "Station USC00092283\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00092485\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00093060\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00093621\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00093842\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00018469\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00013871\n",
      "False    12593\n",
      "dtype: int64\n",
      "()\n",
      "San Francisco stations missing data\n",
      "Station USC00043578\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00045795\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00047916\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00049742\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00023271\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00023272\n",
      "False    12773\n",
      "dtype: int64\n",
      "()\n",
      "San Diego stations missing data\n",
      "Station USC00040983\n",
      "False    12109\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00042239\n",
      "False    12109\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00043914\n",
      "False    12109\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00044223\n",
      "False    12109\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00003164\n",
      "False    12109\n",
      "dtype: int64\n",
      "()\n",
      "Los Angeles stations missing data\n",
      "Station USC00041194\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00046624\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00046719\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00047888\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00049152\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00049785\n",
      "False    12106\n",
      "dtype: int64\n",
      "()\n",
      "Chicago stations missing data\n",
      "Station USC00472869\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00473058\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00473453\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00476200\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USW00014839\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00125174\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n",
      "Station USC00113262\n",
      "False    12652\n",
      "dtype: int64\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(\"New York City stations missing data\")\n",
    "for station in NYC_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(NYC_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Atlanta stations missing data\")\n",
    "for station in ATL_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(ATL_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "    \n",
    "print(\"San Francisco stations missing data\")\n",
    "for station in SF_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(SF_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"San Diego stations missing data\")\n",
    "for station in SD_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(SD_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Los Angeles stations missing data\")\n",
    "for station in LA_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(LA_weather)['TMIN_'+station].value_counts())\n",
    "    print()\n",
    "\n",
    "print(\"Chicago stations missing data\")\n",
    "for station in CH_stations:\n",
    "    print(\"Station %s\" % station)\n",
    "    print(pd.isnull(CH_weather)['TMIN_'+station].value_counts())\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge Datasets from each stock with PARAM_STOCK-NAME as default columns           \n",
    "######Stock Data######\n",
    "stocks = ['Apple, Inc Stock','International Business Machines Stock','Wal-Mart Stores, Inc Common St Stock','FedEx Corporation','The Boeing Company']\n",
    "stock_data = pd.DataFrame()\n",
    "for stock in stocks:\n",
    "    path = 'Stock Data/%s.csv' % stock\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path)\n",
    "        frame.columns = ['DATE','OPEN_'+stock,'HIGH_'+stock,'LOW_'+stock,'CLOSE_'+stock,'VOLUME_'+stock,'ADJ CLOSE_'+stock]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        for column in frame.columns:\n",
    "            if column != 'DATE':\n",
    "                frame['PRE'+column] = frame[column].shift(-1)\n",
    "        if stock_data.empty:\n",
    "            stock_data = stock_data.append(frame,ignore_index=True)\n",
    "        else:\n",
    "            stock_data = stock_data.merge(frame, on='DATE', how='inner')\n",
    "stock_data = stock_data.groupby('DATE').mean()\n",
    "stock_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'DATE', u'OPEN_Apple, Inc Stock', u'HIGH_Apple, Inc Stock',\n",
       "       u'LOW_Apple, Inc Stock', u'CLOSE_Apple, Inc Stock',\n",
       "       u'VOLUME_Apple, Inc Stock', u'ADJ CLOSE_Apple, Inc Stock',\n",
       "       u'OPEN_International Business Machines Stock',\n",
       "       u'HIGH_International Business Machines Stock',\n",
       "       u'LOW_International Business Machines Stock',\n",
       "       u'CLOSE_International Business Machines Stock',\n",
       "       u'VOLUME_International Business Machines Stock',\n",
       "       u'ADJ CLOSE_International Business Machines Stock',\n",
       "       u'OPEN_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'HIGH_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'LOW_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'CLOSE_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'VOLUME_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'ADJ CLOSE_Wal-Mart Stores, Inc Common St Stock',\n",
       "       u'OPEN_FedEx Corporation', u'HIGH_FedEx Corporation',\n",
       "       u'LOW_FedEx Corporation', u'CLOSE_FedEx Corporation',\n",
       "       u'VOLUME_FedEx Corporation', u'ADJ CLOSE_FedEx Corporation',\n",
       "       u'OPEN_The Boeing Company', u'HIGH_The Boeing Company',\n",
       "       u'LOW_The Boeing Company', u'CLOSE_The Boeing Company',\n",
       "       u'VOLUME_The Boeing Company', u'ADJ CLOSE_The Boeing Company'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Merge Datasets from each stock with CITY-NAME_sunlight as default columns\n",
    "#####Sunlight Data######\n",
    "cities = ['ATL_sunlight','NYC_sunlight','LA_sunlight','SF_sunlight','SD_sunlight','CH_sunlight']\n",
    "sunlight_data = pd.DataFrame()\n",
    "for city in cities:\n",
    "    path = 'Sunlight_data/%s.csv' % city\n",
    "    if os.path.exists(path):\n",
    "        frame = pd.read_csv(path,delim_whitespace=True,keep_date_col=True)\n",
    "        frame = frame.iloc[:,0:2]\n",
    "        frame.columns = ['DATE',city]\n",
    "        frame['DATE'] = pd.to_datetime(frame['DATE'])\n",
    "        if sunlight_data.empty:\n",
    "            sunlight_data = sunlight_data.append(frame,ignore_index=True)\n",
    "        else:\n",
    "            sunlight_data = sunlight_data.merge(frame, on='DATE', how='inner')\n",
    "sunlight_data = sunlight_data.groupby('DATE').mean()\n",
    "sunlight_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>OPEN_Apple, Inc Stock</th>\n",
       "      <th>HIGH_Apple, Inc Stock</th>\n",
       "      <th>LOW_Apple, Inc Stock</th>\n",
       "      <th>CLOSE_Apple, Inc Stock</th>\n",
       "      <th>VOLUME_Apple, Inc Stock</th>\n",
       "      <th>ADJ CLOSE_Apple, Inc Stock</th>\n",
       "      <th>PREOPEN_Apple, Inc Stock</th>\n",
       "      <th>PREHIGH_Apple, Inc Stock</th>\n",
       "      <th>PRELOW_Apple, Inc Stock</th>\n",
       "      <th>...</th>\n",
       "      <th>LOW_The Boeing Company</th>\n",
       "      <th>CLOSE_The Boeing Company</th>\n",
       "      <th>VOLUME_The Boeing Company</th>\n",
       "      <th>ADJ CLOSE_The Boeing Company</th>\n",
       "      <th>PREOPEN_The Boeing Company</th>\n",
       "      <th>PREHIGH_The Boeing Company</th>\n",
       "      <th>PRELOW_The Boeing Company</th>\n",
       "      <th>PRECLOSE_The Boeing Company</th>\n",
       "      <th>PREVOLUME_The Boeing Company</th>\n",
       "      <th>PREADJ CLOSE_The Boeing Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1980-12-12</td>\n",
       "      <td>28.749840</td>\n",
       "      <td>28.874720</td>\n",
       "      <td>28.749840</td>\n",
       "      <td>28.749840</td>\n",
       "      <td>117258400</td>\n",
       "      <td>0.438205</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>36.625028</td>\n",
       "      <td>36.999991</td>\n",
       "      <td>816600</td>\n",
       "      <td>2.457737</td>\n",
       "      <td>36.999991</td>\n",
       "      <td>36.999991</td>\n",
       "      <td>36.124988</td>\n",
       "      <td>36.874978</td>\n",
       "      <td>1828200</td>\n",
       "      <td>2.449433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-12-15</td>\n",
       "      <td>27.375041</td>\n",
       "      <td>27.375041</td>\n",
       "      <td>27.250160</td>\n",
       "      <td>27.250160</td>\n",
       "      <td>43971200</td>\n",
       "      <td>0.415346</td>\n",
       "      <td>28.749840</td>\n",
       "      <td>28.874720</td>\n",
       "      <td>28.749840</td>\n",
       "      <td>...</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>37.250009</td>\n",
       "      <td>1157400</td>\n",
       "      <td>2.474345</td>\n",
       "      <td>36.874978</td>\n",
       "      <td>37.250009</td>\n",
       "      <td>36.625028</td>\n",
       "      <td>36.999991</td>\n",
       "      <td>816600</td>\n",
       "      <td>2.457737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-12-16</td>\n",
       "      <td>25.374720</td>\n",
       "      <td>25.374720</td>\n",
       "      <td>25.249840</td>\n",
       "      <td>25.249840</td>\n",
       "      <td>26432000</td>\n",
       "      <td>0.384858</td>\n",
       "      <td>27.375041</td>\n",
       "      <td>27.375041</td>\n",
       "      <td>27.250160</td>\n",
       "      <td>...</td>\n",
       "      <td>36.749969</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>1181000</td>\n",
       "      <td>2.466041</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>37.624972</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>37.250009</td>\n",
       "      <td>1157400</td>\n",
       "      <td>2.474345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-12-17</td>\n",
       "      <td>25.874799</td>\n",
       "      <td>26.000240</td>\n",
       "      <td>25.874799</td>\n",
       "      <td>25.874799</td>\n",
       "      <td>21610400</td>\n",
       "      <td>0.394383</td>\n",
       "      <td>25.374720</td>\n",
       "      <td>25.374720</td>\n",
       "      <td>25.249840</td>\n",
       "      <td>...</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>38.375034</td>\n",
       "      <td>1158200</td>\n",
       "      <td>2.549075</td>\n",
       "      <td>37.250009</td>\n",
       "      <td>37.375022</td>\n",
       "      <td>36.749969</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>1181000</td>\n",
       "      <td>2.466041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-12-18</td>\n",
       "      <td>26.625201</td>\n",
       "      <td>26.750080</td>\n",
       "      <td>26.625201</td>\n",
       "      <td>26.625201</td>\n",
       "      <td>18362400</td>\n",
       "      <td>0.405821</td>\n",
       "      <td>25.874799</td>\n",
       "      <td>26.000240</td>\n",
       "      <td>25.874799</td>\n",
       "      <td>...</td>\n",
       "      <td>38.125012</td>\n",
       "      <td>38.499975</td>\n",
       "      <td>1458200</td>\n",
       "      <td>2.557374</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>38.375034</td>\n",
       "      <td>37.125000</td>\n",
       "      <td>38.375034</td>\n",
       "      <td>1158200</td>\n",
       "      <td>2.549075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  OPEN_Apple, Inc Stock  HIGH_Apple, Inc Stock  \\\n",
       "0 1980-12-12              28.749840              28.874720   \n",
       "1 1980-12-15              27.375041              27.375041   \n",
       "2 1980-12-16              25.374720              25.374720   \n",
       "3 1980-12-17              25.874799              26.000240   \n",
       "4 1980-12-18              26.625201              26.750080   \n",
       "\n",
       "   LOW_Apple, Inc Stock  CLOSE_Apple, Inc Stock  VOLUME_Apple, Inc Stock  \\\n",
       "0             28.749840               28.749840                117258400   \n",
       "1             27.250160               27.250160                 43971200   \n",
       "2             25.249840               25.249840                 26432000   \n",
       "3             25.874799               25.874799                 21610400   \n",
       "4             26.625201               26.625201                 18362400   \n",
       "\n",
       "   ADJ CLOSE_Apple, Inc Stock  PREOPEN_Apple, Inc Stock  \\\n",
       "0                    0.438205                       NaN   \n",
       "1                    0.415346                 28.749840   \n",
       "2                    0.384858                 27.375041   \n",
       "3                    0.394383                 25.374720   \n",
       "4                    0.405821                 25.874799   \n",
       "\n",
       "   PREHIGH_Apple, Inc Stock  PRELOW_Apple, Inc Stock  \\\n",
       "0                       NaN                      NaN   \n",
       "1                 28.874720                28.749840   \n",
       "2                 27.375041                27.250160   \n",
       "3                 25.374720                25.249840   \n",
       "4                 26.000240                25.874799   \n",
       "\n",
       "                ...                 LOW_The Boeing Company  \\\n",
       "0               ...                              36.625028   \n",
       "1               ...                              37.125000   \n",
       "2               ...                              36.749969   \n",
       "3               ...                              37.125000   \n",
       "4               ...                              38.125012   \n",
       "\n",
       "   CLOSE_The Boeing Company  VOLUME_The Boeing Company  \\\n",
       "0                 36.999991                     816600   \n",
       "1                 37.250009                    1157400   \n",
       "2                 37.125000                    1181000   \n",
       "3                 38.375034                    1158200   \n",
       "4                 38.499975                    1458200   \n",
       "\n",
       "   ADJ CLOSE_The Boeing Company  PREOPEN_The Boeing Company  \\\n",
       "0                      2.457737                   36.999991   \n",
       "1                      2.474345                   36.874978   \n",
       "2                      2.466041                   37.125000   \n",
       "3                      2.549075                   37.250009   \n",
       "4                      2.557374                   37.125000   \n",
       "\n",
       "   PREHIGH_The Boeing Company  PRELOW_The Boeing Company  \\\n",
       "0                   36.999991                  36.124988   \n",
       "1                   37.250009                  36.625028   \n",
       "2                   37.624972                  37.125000   \n",
       "3                   37.375022                  36.749969   \n",
       "4                   38.375034                  37.125000   \n",
       "\n",
       "   PRECLOSE_The Boeing Company  PREVOLUME_The Boeing Company  \\\n",
       "0                    36.874978                       1828200   \n",
       "1                    36.999991                        816600   \n",
       "2                    37.250009                       1157400   \n",
       "3                    37.125000                       1181000   \n",
       "4                    38.375034                       1158200   \n",
       "\n",
       "   PREADJ CLOSE_The Boeing Company  \n",
       "0                         2.449433  \n",
       "1                         2.457737  \n",
       "2                         2.474345  \n",
       "3                         2.466041  \n",
       "4                         2.549075  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.DataFrame.head(sunlight_data)\n",
    "#pd.DataFrame.head(stock_data)\n",
    "#pd.DataFrame.head(NYC_weather)\n",
    "#pd.DataFrame.head(df_final)\n",
    "#stock_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_frames = [ATL_weather,CH_weather,NYC_weather,LA_weather,SD_weather,SF_weather,sunlight_data,stock_data]\n",
    "df_final = reduce(lambda left,right: pd.merge(left,right,how='inner',on='DATE'), data_frames)\n",
    "df_final = df_final.ix[3:]\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  TMAX_USC00040983  TMAX_USC00042239  TMAX_USC00043914  \\\n",
      "TMAX_USC00040983          1.000000          0.909123          0.890611   \n",
      "TMAX_USC00042239          0.909123          1.000000          0.937859   \n",
      "TMAX_USC00043914          0.890611          0.937859          1.000000   \n",
      "TMAX_USC00044223          0.963431          0.899407          0.881659   \n",
      "TMAX_USW00003164          0.900046          0.885791          0.880609   \n",
      "\n",
      "                  TMAX_USC00044223  TMAX_USW00003164  \n",
      "TMAX_USC00040983          0.963431          0.900046  \n",
      "TMAX_USC00042239          0.899407          0.885791  \n",
      "TMAX_USC00043914          0.881659          0.880609  \n",
      "TMAX_USC00044223          1.000000          0.906180  \n",
      "TMAX_USW00003164          0.906180          1.000000  \n"
     ]
    }
   ],
   "source": [
    "SD_TMAX = pd.DataFrame()\n",
    "for column in list(SD_weather.columns.values):\n",
    "    if (re.match('TMAX',column)):\n",
    "        SD_TMAX[column] = SD_weather[column]\n",
    "print(SD_TMAX.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  TMIN_USC00043578  TMIN_USC00045795  TMIN_USC00047916  \\\n",
      "TMIN_USC00043578          1.000000          0.773245          0.875948   \n",
      "TMIN_USC00045795          0.773245          1.000000          0.863843   \n",
      "TMIN_USC00047916          0.875948          0.863843          1.000000   \n",
      "TMIN_USC00049742          0.809673          0.725458          0.824087   \n",
      "TMIN_USW00023271          0.822961          0.772047          0.849067   \n",
      "TMIN_USW00023272          0.774539          0.855064          0.808545   \n",
      "\n",
      "                  TMIN_USC00049742  TMIN_USW00023271  TMIN_USW00023272  \n",
      "TMIN_USC00043578          0.809673          0.822961          0.774539  \n",
      "TMIN_USC00045795          0.725458          0.772047          0.855064  \n",
      "TMIN_USC00047916          0.824087          0.849067          0.808545  \n",
      "TMIN_USC00049742          1.000000          0.933714          0.772974  \n",
      "TMIN_USW00023271          0.933714          1.000000          0.808318  \n",
      "TMIN_USW00023272          0.772974          0.808318          1.000000  \n"
     ]
    }
   ],
   "source": [
    "SF_TMIN = pd.DataFrame()\n",
    "for column in list(SF_weather.columns.values):\n",
    "    if (re.match('TMIN',column)):\n",
    "        SF_TMIN[column] = SF_weather[column]\n",
    "print(SF_TMIN.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SNOW_USC00043578  SNOW_USC00045795  SNOW_USC00047916  \\\n",
      "SNOW_USC00043578               NaN               NaN               NaN   \n",
      "SNOW_USC00045795               NaN          1.000000               NaN   \n",
      "SNOW_USC00047916               NaN               NaN               NaN   \n",
      "SNOW_USC00049742               NaN          0.059015               NaN   \n",
      "SNOW_USW00023271               NaN         -0.000194               NaN   \n",
      "SNOW_USW00023272               NaN               NaN               NaN   \n",
      "\n",
      "                  SNOW_USC00049742  SNOW_USW00023271  SNOW_USW00023272  \n",
      "SNOW_USC00043578               NaN               NaN               NaN  \n",
      "SNOW_USC00045795          0.059015         -0.000194               NaN  \n",
      "SNOW_USC00047916               NaN               NaN               NaN  \n",
      "SNOW_USC00049742          1.000000         -0.000129               NaN  \n",
      "SNOW_USW00023271         -0.000129          1.000000               NaN  \n",
      "SNOW_USW00023272               NaN               NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "SF_SNOW = pd.DataFrame()\n",
    "for column in list(SF_weather.columns.values):\n",
    "    if (re.match('SNOW',column)):\n",
    "        SF_SNOW[column] = SF_weather[column]\n",
    "print(SF_SNOW.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  SNWD_USC00043578  SNWD_USC00045795  SNWD_USC00047916  \\\n",
      "SNWD_USC00043578                 1               NaN               NaN   \n",
      "SNWD_USC00045795               NaN               NaN               NaN   \n",
      "SNWD_USC00047916               NaN               NaN               NaN   \n",
      "SNWD_USC00049742               NaN               NaN               NaN   \n",
      "SNWD_USW00023271               NaN               NaN               NaN   \n",
      "SNWD_USW00023272               NaN               NaN               NaN   \n",
      "\n",
      "                  SNWD_USC00049742  SNWD_USW00023271  SNWD_USW00023272  \n",
      "SNWD_USC00043578               NaN               NaN               NaN  \n",
      "SNWD_USC00045795               NaN               NaN               NaN  \n",
      "SNWD_USC00047916               NaN               NaN               NaN  \n",
      "SNWD_USC00049742               NaN               NaN               NaN  \n",
      "SNWD_USW00023271               NaN               NaN               NaN  \n",
      "SNWD_USW00023272               NaN               NaN               NaN  \n"
     ]
    }
   ],
   "source": [
    "SF_SNWD = pd.DataFrame()\n",
    "for column in list(SF_weather.columns.values):\n",
    "    if (re.match('SNWD',column)):\n",
    "        SF_SNWD[column] = SF_weather[column]\n",
    "print(SF_SNWD.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  PRCP_USC00043578  PRCP_USC00045795  PRCP_USC00047916  \\\n",
      "PRCP_USC00043578          1.000000          0.618600          0.720685   \n",
      "PRCP_USC00045795          0.618600          1.000000          0.750712   \n",
      "PRCP_USC00047916          0.720685          0.750712          1.000000   \n",
      "PRCP_USC00049742          0.812961          0.645877          0.719130   \n",
      "PRCP_USW00023271          0.633869          0.616065          0.607253   \n",
      "PRCP_USW00023272          0.623863          0.612386          0.646948   \n",
      "\n",
      "                  PRCP_USC00049742  PRCP_USW00023271  PRCP_USW00023272  \n",
      "PRCP_USC00043578          0.812961          0.633869          0.623863  \n",
      "PRCP_USC00045795          0.645877          0.616065          0.612386  \n",
      "PRCP_USC00047916          0.719130          0.607253          0.646948  \n",
      "PRCP_USC00049742          1.000000          0.595812          0.546373  \n",
      "PRCP_USW00023271          0.595812          1.000000          0.786289  \n",
      "PRCP_USW00023272          0.546373          0.786289          1.000000  \n"
     ]
    }
   ],
   "source": [
    "SF_PRCP = pd.DataFrame()\n",
    "for column in list(SF_weather.columns.values):\n",
    "    if (re.match('PRCP',column)):\n",
    "        SF_PRCP[column] = SF_weather[column]\n",
    "print(SF_PRCP.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Documents/Weather Data'\n",
      "/home/athena/Documents/Weather Data\n"
     ]
    }
   ],
   "source": [
    "cd Documents/Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3       263.833333\n",
      "4       213.000000\n",
      "5       188.833333\n",
      "6       222.000000\n",
      "7       248.166667\n",
      "           ...    \n",
      "6830    192.500000\n",
      "6831    202.666667\n",
      "6832    180.500000\n",
      "6833    158.333333\n",
      "6834    156.666667\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "TMAX_AVG = [0 for x in xrange(len(df_final.index))]\n",
    "zipped = zip(list(df_final.columns.values),TMAX_AVG)\n",
    "count=0\n",
    "for i in list(df_final.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_AVG=TMAX_AVG+df_final.ix[:,i]\n",
    "\n",
    "\n",
    "################################################333\n",
    "TMAX_ATL = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_ATL=TMAX_ATL+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_ATL= TMAX_ATL/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_CH = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_CH=TMAX_CH+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_CH= TMAX_CH/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMAX_LA = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_LA=TMAX_LA+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_LA= TMAX_LA/count\n",
    "print TMAX_AVG_LA\n",
    "\n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_NYC = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_NYC=TMAX_NYC+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_NYC= TMAX_NYC/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMAX_SD = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_SD=TMAX_SD+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_SD= TMAX_SD/count\n",
    "###################################################3\n",
    "################################################333\n",
    "TMAX_SF = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('TMAX',i)):\n",
    "        count=count+1\n",
    "        TMAX_SF=TMAX_SF+df_final.ix[:,i]\n",
    "\n",
    "TMAX_AVG_SF= TMAX_SF/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_ATL = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_ATL=TMIN_ATL+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_ATL= TMIN_ATL/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_CH = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_CH=TMIN_CH+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_CH= TMIN_CH/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_LA = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_LA=TMIN_LA+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_LA= TMIN_LA/count\n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_NYC = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_NYC=TMIN_NYC+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_NYC= TMIN_NYC/count \n",
    "###################################################3\n",
    "################################################333\n",
    "\n",
    "TMIN_SD = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_SD=TMIN_SD+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_SD= TMIN_SD/count \n",
    "###################################################3\n",
    "################################################333\n",
    "TMIN_SF = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('TMIN',i)):\n",
    "        count=count+1\n",
    "        TMIN_SF=TMIN_SF+df_final.ix[:,i]\n",
    "\n",
    "TMIN_AVG_SF= TMIN_SF/count \n",
    "###################################################3\n",
    "##################################################\n",
    "\n",
    "SNOW_ATL = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_ATL=SNOW_ATL+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_ATL= SNOW_ATL/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNOW_NYC = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_NYC=SNOW_NYC+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_NYC= SNOW_NYC/count \n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_CH = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_CH=SNOW_CH+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_CH= SNOW_CH/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_LA = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_LA=SNOW_LA+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_LA= SNOW_LA/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_SD = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_SD=SNOW_SD+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_SD= SNOW_SD/count\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNOW_SF = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('SNOW',i)):\n",
    "        count=count+1\n",
    "        SNOW_SF=SNOW_SF+df_final.ix[:,i]\n",
    "\n",
    "SNOW_AVG_SF= SNOW_SF/count \n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "\n",
    "SNWD_ATL = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_ATL=SNWD_ATL+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_ATL= SNWD_ATL/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_NYC = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_NYC=SNWD_NYC+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_NYC= SNWD_NYC/count\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_CH = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_CH=SNWD_CH+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_CH= SNWD_CH/count \n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_LA = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_LA=SNWD_LA+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_LA= SNWD_LA/count \n",
    "\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "SNWD_SD = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_SD=SNWD_SD+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_SD= SNWD_SD/count \n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "SNWD_SF = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('SNWD',i)):\n",
    "        count=count+1\n",
    "        SNWD_SF=SNWD_SF+df_final.ix[:,i]\n",
    "\n",
    "SNWD_AVG_SF= SNWD_SF/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "PRCP_NYC = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(NYC_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_NYC=PRCP_NYC+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_NYC= PRCP_NYC/count\n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_ATL = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(ATL_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_ATL=PRCP_ATL+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_ATL= PRCP_ATL/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_CH = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(CH_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_CH=PRCP_CH+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_CH= PRCP_CH/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_LA = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(LA_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_LA=PRCP_LA+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_LA= PRCP_LA/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_SD = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SD_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_SD=PRCP_SD+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_SD= PRCP_SD/count \n",
    "\n",
    "#####################################################\n",
    "#####################################################\n",
    "\n",
    "PRCP_SF = [0 for x in xrange(len(df_final.index))]\n",
    "count=0\n",
    "for i in list(SF_weather.columns.values):\n",
    "    if(re.match('PRCP',i)):\n",
    "        count=count+1\n",
    "        PRCP_SF=PRCP_SF+df_final.ix[:,i]\n",
    "\n",
    "PRCP_AVG_SF= PRCP_SF/count \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"TMAX_ATL\"]=TMAX_AVG_ATL\n",
    "df[\"TMAX_NYC\"]=TMAX_AVG_NYC\n",
    "df[\"TMAX_CH\"]=TMAX_AVG_CH\n",
    "df[\"TMAX_LA\"]=TMAX_AVG_LA\n",
    "df[\"TMAX_SD\"]=TMAX_AVG_SD\n",
    "df[\"TMAX_SF\"]=TMAX_AVG_SF\n",
    "\n",
    "\n",
    "df[\"TMIN_ATL\"]=TMIN_AVG_ATL\n",
    "df[\"TMIN_NYC\"]=TMIN_AVG_NYC\n",
    "df[\"TMIN_CH\"]=TMIN_AVG_CH\n",
    "df[\"TMIN_LA\"]=TMIN_AVG_LA\n",
    "df[\"TMIN_SD\"]=TMIN_AVG_SD\n",
    "df[\"TMIN_SF\"]=TMIN_AVG_SF \n",
    "\n",
    "df[\"SNOW_ATL\"]=SNOW_AVG_ATL\n",
    "df[\"SNOW_NYC\"]=SNOW_AVG_NYC\n",
    "df[\"SNOW_CH\"]=SNOW_AVG_CH\n",
    "df[\"SNOW_LA\"]=SNOW_AVG_LA\n",
    "df[\"SNOW_SD\"]=SNOW_AVG_SD\n",
    "df[\"SNOW_SF\"]=SNOW_AVG_SF \n",
    "\n",
    "df[\"SNWD_ATL\"]=SNWD_AVG_ATL\n",
    "df[\"SNWD_NYC\"]=SNWD_AVG_NYC\n",
    "df[\"SNWD_CH\"]=SNWD_AVG_CH\n",
    "df[\"SNWD_LA\"]=SNWD_AVG_LA\n",
    "df[\"SNWD_SD\"]=SNWD_AVG_SD\n",
    "df[\"SNWD_SF\"]=SNWD_AVG_SF \n",
    "\n",
    "df[\"PRCP_ATL\"]=PRCP_AVG_ATL\n",
    "df[\"PRCP_NYC\"]=PRCP_AVG_NYC\n",
    "df[\"PRCP_CH\"]=PRCP_AVG_CH\n",
    "df[\"PRCP_LA\"]=PRCP_AVG_LA\n",
    "df[\"PRCP_SD\"]=PRCP_AVG_SD\n",
    "df[\"PRCP_SF\"]=PRCP_AVG_SF\n",
    "\n",
    "df[\"SUN_ATL\"]=df_final[\"ATL_sunlight\"]\n",
    "df[\"SUN_NYC\"]=df_final[\"NYC_sunlight\"]\n",
    "df[\"SUN_LA\"]=df_final[\"LA_sunlight\"]\n",
    "df[\"SUN_CH\"]=df_final[\"CH_sunlight\"]\n",
    "df[\"SUN_SD\"]=df_final[\"SD_sunlight\"]\n",
    "df[\"SUN_SF\"]=df_final[\"SF_sunlight\"]\n",
    "\n",
    "##Adding day column##\n",
    "from datetime import date\n",
    "import calendar\n",
    "date.today().strftime(\"%A\")\n",
    "dx = {}\n",
    "for date in df_final[\"DATE\"]:\n",
    "        dx[date] = date.strftime(\"%A\")\n",
    "import collections\n",
    "dx = collections.OrderedDict(sorted(dx.items()))\n",
    "df_final[\"DAY\"] = dx.values()\n",
    "df_final[\"DAY\"] = df_final[\"DAY\"].astype('category')\n",
    "df = pd.concat([df, pd.get_dummies(df_final[\"DAY\"])],axis =1) \n",
    "####\n",
    "\n",
    "df[\"PreWalMart\"]= (-np.log(df_final[\"PREOPEN_Wal-Mart Stores, Inc Common St Stock\"])+ np.log(df_final[\"PRECLOSE_Wal-Mart Stores, Inc Common St Stock\"]))*100*df_final[\"PREVOLUME_Wal-Mart Stores, Inc Common St Stock\"]  \n",
    "\n",
    "df[\"PreApple\"]= (-np.log(df_final[\"PREOPEN_Apple, Inc Stock\"])+ np.log(df_final[\"PRECLOSE_Apple, Inc Stock\"]))*100*df_final[\"PREVOLUME_Apple, Inc Stock\"]\n",
    "                                                               \n",
    "df[\"PreBoeing\"]= (-np.log(df_final[\"PREOPEN_The Boeing Company\"])+ np.log(df_final[\"PRECLOSE_The Boeing Company\"]))*100*df_final[\"PREVOLUME_The Boeing Company\"]\n",
    "\n",
    "df[\"PreFedEx\"]= (-np.log(df_final[\"PREOPEN_FedEx Corporation\"])+ np.log(df_final[\"PRECLOSE_FedEx Corporation\"]))*100*df_final[\"PREVOLUME_FedEx Corporation\"]\n",
    "\n",
    "df[\"PreIBM\"]= (-np.log(df_final[\"PREOPEN_International Business Machines Stock\"])+ np.log(df_final[\"PRECLOSE_International Business Machines Stock\"]))*100*df_final[\"PREVOLUME_International Business Machines Stock\"]\n",
    "\n",
    "df[\"WalMart\"]= (-np.log(df_final[\"OPEN_Wal-Mart Stores, Inc Common St Stock\"])+ np.log(df_final[\"CLOSE_Wal-Mart Stores, Inc Common St Stock\"]))*100*df_final[\"VOLUME_Wal-Mart Stores, Inc Common St Stock\"]  \n",
    "\n",
    "df[\"Apple\"]= (-np.log(df_final[\"OPEN_Apple, Inc Stock\"])+ np.log(df_final[\"CLOSE_Apple, Inc Stock\"]))*100*df_final[\"VOLUME_Apple, Inc Stock\"]\n",
    "                                                               \n",
    "df[\"Boeing\"]= (-np.log(df_final[\"OPEN_The Boeing Company\"])+ np.log(df_final[\"CLOSE_The Boeing Company\"]))*100*df_final[\"VOLUME_The Boeing Company\"]\n",
    "\n",
    "df[\"FedEx\"]= (-np.log(df_final[\"OPEN_FedEx Corporation\"])+ np.log(df_final[\"CLOSE_FedEx Corporation\"]))*100*df_final[\"VOLUME_FedEx Corporation\"]\n",
    "\n",
    "df[\"IBM\"]= (-np.log(df_final[\"OPEN_International Business Machines Stock\"])+ np.log(df_final[\"CLOSE_International Business Machines Stock\"]))*100*df_final[\"VOLUME_International Business Machines Stock\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "res = defaultdict(dict)\n",
    "\n",
    "X = pd.DataFrame(df.ix[:,0:42])\n",
    "Y = pd.DataFrame(df.ix[:,42:47])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y.ix[:,0], test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_features': 0.3, 'min_samples_split': 10}\n"
     ]
    }
   ],
   "source": [
    "param_grid1 = {'learning_rate': [0.1,0.01],\n",
    "              'max_depth': [1,2],\n",
    "              'max_features': [1]\n",
    "              }\n",
    "param_grid2 = {'max_features' : [0.3,0.5],\n",
    "              'min_samples_split' : [4,10],\n",
    "               }\n",
    "param_grid3 = {'alpha' : [0.01,0.1,1,100,1000],\n",
    "              'l1_ratio' : [0.2,0.4,0.6,0.8,1]}\n",
    "\n",
    "est1 = GradientBoostingRegressor(n_estimators=50)\n",
    "est2 = RandomForestRegressor(n_estimators=200)\n",
    "est3 = ElasticNet()\n",
    "\n",
    "# this may take some minutes\n",
    "gs_cv = GridSearchCV(est2, param_grid2, scoring='mean_squared_error', n_jobs=4).fit(X_train, y_train)\n",
    "\n",
    "# best hyperparameter setting\n",
    "print('Best hyperparameters: %r' % gs_cv.best_params_)\n",
    "#print \"abc\"\n",
    "#regressor = DecisionTreeRegressor(max_depth = 1,random_state=0)\n",
    "#print cross_val_score(regressor, X, Y.ix[:,0], cv=10)\n",
    "\n",
    "# predict class labels\n",
    "#pred = est.predict(X_test)\n",
    "\n",
    "# score on test data (accuracy)\n",
    "#testacc = est.score(X_test, y_test)\n",
    "#trainacc = est.score(X_train,y_train)\n",
    "\n",
    "#print('TestACC: %.4f' % testacc)\n",
    "#print('TrainACC:%.4f' % trainacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.0578\n"
     ]
    }
   ],
   "source": [
    "# refit model on best parameters\n",
    "est2.set_params(**gs_cv.best_params_)\n",
    "est2.fit(X_train, y_train)\n",
    "pred = est2.predict(X_test)\n",
    "acc = est2.score(X_test, y_test)\n",
    "print('ACC: %.4f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
